{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d19f42-e41e-4d92-9a18-1701200069b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f80d8-4dfa-4e12-8210-4e27637be1ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BEFORE YOU START:\n",
    "-  Make sure to download all of the files from the 'OpenCV Notes' folder on GitHub\n",
    "-  Create a folder called 'EOH OpenCV' in Jupyter and put all of the files in that folder\n",
    "      -  a lot of the code tries to access the files from this folder, so the code will often not run if you don't have this folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c47e326-db6e-47ca-85db-9c725b45c03a",
   "metadata": {},
   "source": [
    "## (1) Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a65bdf-b03a-45f1-a47c-5e8c7743232a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(300, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "# images are numpy arrays\n",
    "\n",
    "# create image variable from .jpg\n",
    "image = cv2.imread('test_image.jpg')\n",
    "# if your image is in a folder, you will need to use an image path (shown in more detail in (2))\n",
    "image = cv2.imread('EOH OpenCV/test_image.jpg')\n",
    "print(os.path.exists('Contacts'))\n",
    "\n",
    "\n",
    "# images are given height, width, and number of channels\n",
    "print(image.shape)\n",
    "\n",
    "# images are made of pixels\n",
    "    # has value ranging from 0 to 255 (in most cases)\n",
    "    # in binary images, has value 0 or 1 (or 0 and 255)\n",
    "    # in 16 bit images, value ranges from 0 to 65535\n",
    "# final color is determined by the pixel values in each color channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acad284-7621-436b-bf0b-4f3a8cb9d835",
   "metadata": {},
   "source": [
    "## (2) Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b40f8f-c9f5-4393-8fe5-6db755b2321d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read image (creating variable from image in files)\n",
    "image_path = os.path.join('.', 'EOH OpenCV', 'test_image.jpg')\n",
    "# 'EOH OpenCV' is the name of the folder I put 'test_image.jpg' in\n",
    "# if the image is not in a folder, you only need '.' and 'test_image.jpg'\n",
    "# if the image is inside multiple folders, you need to add those as well\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# write image (saving image with a new name)\n",
    "cv2.imwrite('test_image_2.jpg', image)\n",
    "cv2.imwrite('EOH OpenCV/test_image_2.jpg', image) \n",
    "# visualize image\n",
    "\n",
    "cv2.imshow('window name', image) # frame is the name of the window that the window is shown\n",
    "cv2.waitKey(0) # defines number of ms until window closes, stays open indefinately when 0\n",
    "\n",
    "# this does not work on google colab, for images there is a workaround, but I could not find a workaround for videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e862ed-d521-457d-8c71-d91aa724718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read video\n",
    "video_path = os.path.join('.', 'EOH OpenCV', 'test_video.mp4')\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# visualize video\n",
    "ret = True\n",
    "while ret: # ends loop when ret=False (no more frames to read)\n",
    "    ret, frame = video.read() # reads each frame\n",
    "    if ret:\n",
    "      cv2.imshow('window name', frame)\n",
    "      cv2.waitKey(40)\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9698668b-175e-4454-a603-e9a77d9b535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read webcam\n",
    "webcam = cv2.VideoCapture(0) # number of the webcam you want to access\n",
    "\n",
    "# visualize webcam\n",
    "while True:\n",
    "    ret, frame = webcam.read()\n",
    "    cv2.imshow('window name', frame)\n",
    "    if cv2.waitKey(40) & 0xFF == ord(' '):\n",
    "        break  # stops loop (and webcam video) when the [spacebar] is pressed\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d72eb7-349c-486d-91f6-c5774df59939",
   "metadata": {},
   "source": [
    "## (3) Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5561c078-338b-471e-b2f8-1998ec84ddd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 600, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resizing / cropping\n",
    "image = cv2.imread(os.path.join('.', 'EOH OpenCV', 'test_image.jpg'))\n",
    "print(image.shape)\n",
    "\n",
    "cropped_image = image[50:250, 100:500]\n",
    "\n",
    "cv2.imshow('window name', cropped_image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e562563-860f-4edd-ac55-838ec50a4ebf",
   "metadata": {},
   "source": [
    "## (4) Color Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ff69fe3-1287-41c5-b95f-dfa05936750a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(os.path.join('.', 'EOH OpenCV', 'test_image.jpg'))\n",
    "\n",
    "# convert image between color spaces\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to RGB color space\n",
    "\n",
    "cv2.imshow('BGR color space (normal)', image) # display normal image\n",
    "cv2.imshow('RGB color space', image_rgb) # display image in new color space\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# color spaces change how the image is displayed, but it's given all the same information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83b2c3ce-b060-40c1-a0e3-b629e7ba9cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more color spaces\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # converting to Grayscale color space\n",
    "image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) # converting to HSV color space\n",
    "\n",
    "cv2.imshow('BGR color space (normal)', image) # display normal image\n",
    "cv2.imshow('RGB color space', image_rgb)\n",
    "cv2.imshow('Grayscale color space', image_gray)\n",
    "cv2.imshow('HSV color space', image_hsv)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# grayscale only has one color channel, while normally its 3\n",
    "\n",
    "# hsv is made for computer detection and is popular in computer vision, but it doesn't make sense to look at as a human\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f757a1-d31d-4738-abf5-e651a1bc2d10",
   "metadata": {},
   "source": [
    "## (5) Blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2125fb4-afa7-4043-aa8f-e37b700bf355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used often to remove noise\n",
    "# 4 funtions you can use: blur(), GaussianBlur(), medianBlur(), bilateralFilter()\n",
    "# blurring replaces pixels with the average value of nearby pixels\n",
    "\n",
    "# blur()\n",
    "image = cv2.imread(os.path.join('.', 'EOH OpenCV', 'test_image.jpg'))\n",
    "image_blur = cv2.blur(image, (5, 5)) # numbers define how many nearby pixels you are using for the average value of each pixel\n",
    "    # higher numbers -> bigger area -> more blur\n",
    "\n",
    "cv2.imshow('normal', image)\n",
    "cv2.imshow('blur()', image_blur)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3efb5b-ed61-4923-8dee-43d94537f455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GaussianBlur() and medianBlur\n",
    "image_GausBlur = cv2.GaussianBlur(image, (5, 5), 3) # additional parameter is needed\n",
    "image_medBlur = cv2.medianBlur(image, 5)\n",
    "cv2.imshow('normal', image)\n",
    "cv2.imshow('blur()', image_blur)\n",
    "cv2.imshow('GaussianBlur()', image_GausBlur)\n",
    "cv2.imshow('medianBlur()', image_medBlur)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# each blur is slightly different and has different use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e7231-4238-4c42-9786-e7d0707d5593",
   "metadata": {},
   "source": [
    "## (6) Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e62e3d-12c2-4d4a-ad24-f0148022986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common use-case is to make a binary image\n",
    "# used to separate image from the background\n",
    "\n",
    "image = cv2.imread(os.path.join('.', 'EOH OpenCV', 'test_image.jpg'))\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "\n",
    "X = 80\n",
    "ret, image_thresh = cv2.threshold(image_gray, X, 255, cv2.THRESH_BINARY)\n",
    "    #  X is the (simple) threshold value\n",
    "        # values below X go to 0 and above X go to 255\n",
    "\n",
    "cv2.imshow('grayscale', image_gray)\n",
    "cv2.imshow('threshold', image_thresh)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f7ca4fe-d8a0-4789-9796-d178cbe33444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# threshold with blur\n",
    "\n",
    "image_thresh = cv2.blur(image_thresh, (5, 5))\n",
    "ret, image_thresh = cv2.threshold(image_thresh, X, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "cv2.imshow('grayscale', image_gray)\n",
    "cv2.imshow('threshold', image_thresh)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9e22f92-a601-4a1a-bd53-5e3bc6e89fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adaptive Threshold\n",
    "# used for certain images where we cannot use one simple threshold value to get a clear image\n",
    "\n",
    "# if we use simple threshold for certain images:\n",
    "image = cv2.imread(os.path.join('.', 'EOH OpenCV', 'thresh_image.jpg')) # new image\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "\n",
    "ret, image_thresh = cv2.threshold(image_gray, 60, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "cv2.imshow('grayscale', image_gray)\n",
    "cv2.imshow('simple threshold', image_thresh)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f8c5845-4956-4674-a26c-34818b786b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Adaptive Threshold\n",
    "image_adapt = cv2.adaptiveThreshold(image_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 30)\n",
    "    # will make many thresholds for different sections of the image\n",
    "    # cuts image into regions and finds optimal threshold for each region\n",
    "\n",
    "cv2.imshow('grayscale', image_gray)\n",
    "cv2.imshow('adaptive threshold', image_adapt)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b20a6a-2616-4f32-897e-32de63634a82",
   "metadata": {},
   "source": [
    "## (7) Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3dec5b3-ce9a-49a6-8298-46f6d0eec91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 types of edge detectors: Sobel, Laplace, Canny\n",
    "# Canny Edge Detector\n",
    "image = cv2.imread(os.path.join('.', 'EOH OpenCV', 'test_image.jpg'))\n",
    "\n",
    "image_edge = cv2.Canny(image, 350, 600)\n",
    "    # two numbers affect how it determines an edge\n",
    "\n",
    "cv2.imshow('normal', image)\n",
    "cv2.imshow('edge detection', image_edge)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "984041ae-6533-4077-a4aa-e98bc8768218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dilating and eroding\n",
    "image = cv2.imread(os.path.join('.', 'EOH OpenCV', 'test_image.jpg'))\n",
    "\n",
    "image_edge = cv2.Canny(image, 350, 600)\n",
    "\n",
    "image_edge_d = cv2.dilate(image_edge, np.ones((2, 2), dtype=np.int8))\n",
    "    # dialating makes all the borders thicker, bigger #s in np.ones() -> thicker lines\n",
    "\n",
    "image_edge_e = cv2.erode(image_edge_d, np.ones((2, 2), dtype=np.int8))\n",
    "    # eroding does the opposite of dialting, makes lines thinner\n",
    "\n",
    "cv2.imshow('normal', image)\n",
    "cv2.imshow('edge detection', image_edge)\n",
    "cv2.imshow('edge detection dialated', image_edge_d)\n",
    "cv2.imshow('edge detection eroded', image_edge_e)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c4f52-d907-486e-a298-79d866e9ab2c",
   "metadata": {},
   "source": [
    "## (8) Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cace4af3-990e-4dc3-9b8c-7a929132a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 600, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(os.path.join('.', 'EOH OpenCV', 'test_image.jpg'))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "ret, image = cv2.threshold(image, -1, 100, cv2.THRESH_BINARY)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    # dumb way of making a plain gray image to draw on\n",
    "\n",
    "# line\n",
    "print(image.shape)\n",
    "cv2.line(image, (100, 100), (500, 200), (0, 255, 0), 3)\n",
    "    # parameters: image, (x1, y1), (x2, y2), color, thickness\n",
    "    # define the two points that the line should go across with (x1, y1) and (x2, y2)\n",
    "\n",
    "# Note: image.shape gives points in (y, x), while line takes points in (x, y)\n",
    "\n",
    "cv2.imshow('window', image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "993c9691-8efc-4246-8e8d-67599727e1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 600, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rectangle\n",
    "print(image.shape)\n",
    "cv2.rectangle(image, (150, 50), (400, 150), (255, 0, 0), 6)\n",
    "    # parameters: image, (x1, y1), (x2, y2), color, thickness\n",
    "    # (x1, y1) is the upper left point, (x2, y2) is bottom right\n",
    "\n",
    "cv2.imshow('window', image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fcf0d92-9247-4503-9d0c-02abc9e97614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 600, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# circle\n",
    "print(image.shape)\n",
    "cv2.circle(image, (180, 280), 100, (0, 0, 255), -1)\n",
    "    # parameters: image, (x, y), radius, color, thickness\n",
    "    # (x, y) is the center of the circle\n",
    "    # thickness of -1 fills in the shape\n",
    "    # shapes are allowed to go off the bounds of the image\n",
    "\n",
    "cv2.imshow('window', image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93e6d21c-3bb1-4b4c-afcf-9335a28742a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 600, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text\n",
    "print(image.shape)\n",
    "cv2.putText(image, 'hello world!', (250, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 25, 100), 4)\n",
    "    # parameters: image, text, (x, y), font, size, color, thickness\n",
    "    # (x, y) is the bottom left corner of the text\n",
    "\n",
    "\n",
    "cv2.imshow('window', image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1a29a-8f9d-4f0b-b3bf-fc20440586e4",
   "metadata": {},
   "source": [
    "## (9) Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30b40670-1491-4a71-83b1-0b3a2bca3eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(os.path.join('.', 'EOH OpenCV', 'birds.png'))\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret, image_thresh = cv2.threshold(image_gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "    # threshold that takes a threshold and inverts the image afterward\n",
    "\n",
    "# need black and white image for contours\n",
    "# we need to inverse because we will be dececting the white objects\n",
    "\n",
    "contours, hierarchy = cv2.findContours(image_thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # 'contours' will be a list where each element is an isolated object (isolated white shapes surrounded by black pixels in the image) \n",
    "for cnt in contours:\n",
    "    if cv2.contourArea(cnt) > 200:  # only continues if size of the object is large enough (removes noise)\n",
    "        cv2.drawContours(image, cnt, -1, (0, 0, 255), 1)\n",
    "        # parameters: image, contour, -1, color, thickness\n",
    "        # draws contours on the original image\n",
    "\n",
    "cv2.imshow('image', image)\n",
    "cv2.imshow('threshold', image_thresh)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c80f1946-1cf5-4591-8763-6665aa233701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making boxes around each object\n",
    "for cnt in contours:\n",
    "    if cv2.contourArea(cnt) > 200:  # only continues if size of the object is large enough (removes noise)\n",
    "        x1, y1, w, h = cv2.boundingRect(cnt) # finds a bounding box for each object\n",
    "        cv2.rectangle(image, (x1, y1), (x1 + w, y1 + h), (0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow('image', image)\n",
    "cv2.imshow('threshold', image_thresh)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d6a47-ba9a-4d9b-a794-ff1aaffe9906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
